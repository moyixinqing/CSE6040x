{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Problem 12: Snowball Poem Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "This problem will test your ability to work with Python data structures as well as working with text data. This problem is vaguely based on work by [*Paul Thompson*](https://github.com/nossidge). (However, you won't find the answer on his GitHub.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We will work through the creation of a **Snowball poem generator**. Snowball poems are a type of constraint to poetry belonging to the [Oulipo group](https://en.wikipedia.org/wiki/Oulipo). The Oulipo group is a collection of mathematicians and linguists who create writings based on particular linguistic constraints. Snowball poems are governed by the constraint that each successive word be one letter longer than the previous word, and each word takes up a single line. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "o  \n",
    "we  \n",
    "all  \n",
    "have  \n",
    "heard  \n",
    "people  \n",
    "believe  \n",
    "anything  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "The texts we will use are *Life on the Mississippi*, *Adventures of Huckleberry Finn*, and *The Adventures of Tom Sawyer* by Mark Twain, obtained from *Project Gutenberg*. \n",
    "\n",
    "For this task, much like our Shakespeare generator on the first midterm, we also include the constraint that each word must follow the preceding word at some point in the source text. \n",
    "\n",
    "We have cleaned and prepared the data for you, removing all numbers, punctuation outside of pre-determined \"stop characters\", \".!?;:\", all chapter headers, and filtering out all non-English words. \n",
    "\n",
    "Run the following code cell to read in the data and display the first 100 characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mississippi is well worth reading about. it is not a commonplace river but on the contrary is in\n",
      "you dont know about me without you have read a book by the name of the adventures of tom sawyer; but\n",
      "tom! no answer. tom! no answer. whats gone with that boy i wonder? you tom! no answer. the old lady \n"
     ]
    }
   ],
   "source": [
    "life_on_miss = open(\"./resource/asnlib/publicdata/LifeOnMississippiCleaned.txt\", \"r\").read().lower()\n",
    "huck_finn = open(\"./resource/asnlib/publicdata/HuckFinnCleaned.txt\", \"r\").read().lower()\n",
    "tom_saw = open(\"./resource/asnlib/publicdata/TomSawyerCleaned.txt\", \"r\").read().lower()\n",
    "print(life_on_miss[:100])\n",
    "print(huck_finn[:100])\n",
    "print(tom_saw[:100])\n",
    "\n",
    "full_text = life_on_miss + \" \" + huck_finn + \" \" + tom_saw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Exercise 0 (3 pts):** You will now create the first data structure, `snowball_mapping`, a `defaultdict` mapping a word of length `n`, to a set of all words following it in the text that are of length `n + 1`. In other words, each key should be a word, and the values should be the list of words that follow the key word in the source text *and* are one character longer than the key word.\n",
    "\n",
    "For example, if our source text was:\n",
    "\n",
    "```python\n",
    "'This is the full source material. It is not a full novel written by the very esteemed Mark Twain. Woohoo abcdefg.'\n",
    "```\n",
    "\n",
    "`snowball_mapping` will look like:\n",
    "\n",
    "```python \n",
    "snowball_mapping[\"is\"] = {\"the\", \"not\"}\n",
    "snowball_mapping[\"the\"] = {\"full\", \"very\"}\n",
    "snowball_mapping[\"full\"] = {\"novel\"}\n",
    "snowball_mapping[\"by\"] = {\"the\"}\n",
    "snowball_mapping[\"Mark\"] = {\"Twain\"}\n",
    "snowball_mapping[\"Woohoo\"] = {\"abcdefg\"}\n",
    "```\n",
    "\n",
    "There are two important details to mention. The first is: do not include words in `snowball_mapping` that have no words of length `n + 1` following in the text. For instance, above, you'll see there is no entry for the five-letter word \"novel\" in the snowball_mapping dictionary because the word that comes after it, \"written\", is seven characters long. \n",
    "\n",
    "The next important detail is: if a word is followed by a \"stop character\" (.!?;:), don't add the next word to the values for the first word. For instance, notice above that there is no key/value pair of \"Twain\" : {\"Woohoo\"}, because there is a \".\" between them. You are to consider each sentence (split by \".!?;:\") separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "import numpy as np\n",
    "snowball_mapping = defaultdict(set)\n",
    "\n",
    "###\n",
    "\n",
    "# t = 'This is the full source material. It is not a full novel written by the very esteemed Mark Twain. Woohoo abcdefg.'\n",
    "t = full_text\n",
    "sep = [x.strip() for x in re.split('[.!?;:]+',t.strip())]\n",
    "for se in sep:\n",
    "    li = se.split()\n",
    "    for i in range(len(li)-1):\n",
    "            if len(li[i])+1 == len(li[i+1]):\n",
    "                snowball_mapping[li[i]].add(li[i+1])\n",
    "# snowball_mapping\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "test_snowball_mapping",
     "locked": true,
     "points": "3",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, is your dictionary the right size?\n",
      "\n",
      "Looks like it is -- what about the format? Are the values sets of words one letter longer than the keys?\n",
      "\n",
      "Looks good! And finally, to test out a few particular key/value pairs....\n",
      "\n",
      "Passed!\n"
     ]
    }
   ],
   "source": [
    "# test_snowball_mapping\n",
    "import random\n",
    "\n",
    "print(\"First, is your dictionary the right size?\")\n",
    "\n",
    "assert len(snowball_mapping.items()) <= 2291, \"Your dictionary has too many items!\"\n",
    "assert len(snowball_mapping.items()) >= 2291, \"Your dictionary has too few items!\"\n",
    "\n",
    "print(\"\\nLooks like it is -- what about the format? Are the values sets of words one letter longer than the keys?\")\n",
    "\n",
    "for iter_num in range(100):\n",
    "    test_key = random.choice(list(snowball_mapping))\n",
    "\n",
    "    for word in snowball_mapping[test_key]:\n",
    "\n",
    "        assert set(snowball_mapping[test_key]) == snowball_mapping[test_key], \"The values in your dictionary aren't sets.\"\n",
    "        assert len(test_key) == len(word) - 1, \"Looks like you've got some words that are the wrong length for their key word.\"\n",
    "\n",
    "print(\"\\nLooks good! And finally, to test out a few particular key/value pairs....\\n\")\n",
    "        \n",
    "assert snowball_mapping['tootwo'] == {'hundred'}\n",
    "assert snowball_mapping['sneeze'] == {'started'}\n",
    "assert snowball_mapping['astonishing'] == {'constitution'}\n",
    "assert snowball_mapping['candles'] == {'revealed'}\n",
    "        \n",
    "print(\"Passed!\")\n",
    "## snowball_mapping test cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Exercise 1 (3 pts):** One addition we will make to the standard Snowball poem, is the idea of the longest \"natural\" run for each word. We define a natural run as a sequence of words that all fulfill the Snowball property in the original text. So for a given word, `w`, we wish to find the longest sequence of words starting with `w` that fulfill the Snowball property. For example, the sentence:\n",
    "\n",
    "```python\n",
    "'I am his only amigo. Indeed.'\n",
    "```\n",
    "\n",
    "Contains a long \"natural\" run. You will create a `defaultdict`, `longest_natural`, that maps a given word `w` to its **longest** \"natural\" run in the text, represented as a **list**, ordered as it appears in the text. For the sentence above, you will have:\n",
    "\n",
    "```python\n",
    "longest_natural[\"I\"] = [\"am\", \"his\", \"only\", \"amigo\"]\n",
    "longest_natural[\"am\"] = [\"his\", \"only\", \"amigo\"]\n",
    "longest_natural[\"his\"] = [\"only\", \"amigo\"]\n",
    "longest_natural[\"only\"] = [\"amigo\"]\n",
    "```\n",
    "\n",
    "Again, do not include words (with length `n`) that have no words of length `n + 1` following, and you should treat the stop characters as phrase-ending (`\"Indeed\"` not included in the previous example). **If there are ties for longest natural runs for a given word, keep the first instance that occurs while iterating through the text**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_natural = defaultdict(list)\n",
    "\n",
    "###\n",
    "# t = 'I am his only amigo. Indeed.'\n",
    "t = full_text\n",
    "sep = [x.strip(' ') for x in re.split('[.!?;:]+',t.strip())]\n",
    "for se in sep:\n",
    "    li = se.split()\n",
    "   \n",
    "    for i in range(len(li)):\n",
    "            j = 0\n",
    "            temp = []\n",
    "            while j+i+1 < len(li) and len(li[i+j])+1 == len(li[i+1+j]):\n",
    "                temp.append(li[i+1+j]) \n",
    "                j += 1\n",
    "            if len(temp) != 0:\n",
    "                if (len(longest_natural[li[i]]) < len(temp)) :\n",
    "                    longest_natural[li[i]] = temp\n",
    "\n",
    "\n",
    "# len(longest_natural)\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "test_longest_natural_1",
     "locked": true,
     "points": "1",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, let's see if your dictionary is the right length.\n",
      "\n",
      "Looks like they're all accounted for. Passed!\n"
     ]
    }
   ],
   "source": [
    "#test_longest_natural_1\n",
    "print(\"First, let's see if your dictionary is the right length.\")\n",
    "\n",
    "assert len(longest_natural) == 2291, \"This dictionary should be 2,291 entries too!\"\n",
    "\n",
    "print(\"\\nLooks like they're all accounted for. Passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "test_longest_natural_2",
     "locked": true,
     "points": "2",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There's a 14-way tie for longest natural run -- let's see if you have them all!\n",
      "\n",
      "i ['do', 'now', 'that', 'there']\n",
      "and ['from', 'posey', 'county', 'indiana']\n",
      "by ['the', 'cold', 'world', 'ragged']\n",
      "of ['the', 'were', 'still', 'belted']\n",
      "if ['she', 'were', 'about', 'scared']\n",
      "no ['use', 'they', 'didnt', 'happen']\n",
      "we ['see', 'aunt', 'sally', 'coming']\n",
      "the ['bend', 'above', 'island', 'brimful']\n",
      "it ['was', 'five', 'years', 'before']\n",
      "had ['been', 'found', 'lodged', 'against']\n",
      "tom ['went', 'about', 'hoping', 'against']\n",
      "log ['raft', 'would', 'appear', 'vaguely']\n",
      "to ['not', 'deny', 'about', 'hiding']\n",
      "in ['the', 'open', 'place', 'before']\n",
      "\n",
      "Now let's see if you have the triple with the longest first word:\n",
      "\n",
      "falsehood: ['undergoing', 'restoration'] \n",
      "\n",
      "Passed!\n"
     ]
    }
   ],
   "source": [
    "#test_longest_natural_2\n",
    "print(\"There's a 14-way tie for longest natural run -- let's see if you have them all!\\n\")\n",
    "long_run_list = [\"i\", \"and\", \"by\", \"of\", \"if\", \"no\", \"we\", \"the\", \"it\", \"had\", \"tom\", \"log\", \"to\", \"in\"]\n",
    "\n",
    "for word in long_run_list:\n",
    "    assert len(longest_natural[word]) == 4, \"Looks like you've got the wrong natural run for one of them.\"\n",
    "    print(word, longest_natural[word])\n",
    "        \n",
    "print(\"\\nNow let's see if you have the triple with the longest first word:\")\n",
    "assert longest_natural['falsehood'] == ['undergoing', 'restoration'], \"Hmm, looks like you didn't get the right result.\"\n",
    "\n",
    "print(\"\\nfalsehood:\", longest_natural['falsehood'], \"\\n\\nPassed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Exercise 2 (4 pts):** You will now put it all together. You will create one function named `snowball_generator(start_word, snowball_mapping, longest_natural, use_natural)`, where `start_word` is the word to start the poem on, `snowball_mapping` is your previously created `defaultdict` containing the length `n` -> `n + 1` mappings, `longest_natural` is your previously created `defaultdict` containing the longest natural runs, and `use_natural` is a binary flag for whether or not to use the natural runs or otherwise. \n",
    "\n",
    "The algorithm depends on the usage of natural runs.\n",
    "\n",
    "* If `use_natural` is False, you will begin with `start_word` and randomly choose a word of `n + 1` length (for `len(start_word) = n`) that follows it from `snowball_mapping`. You will then continue doing this until there are no more words to choose.\n",
    "* If `use_natural` is True, you will begin with `start_word` and use its longest natural run. You will then choose the longest natural run beginning with the final word of `start_word`'s longest natural run, and continue this process until there are no more words to choose.\n",
    "\n",
    "In order to print the poem in an attractive manner, in between each line of the poem, append a newline character, `\\n` to your string, so the poem from above:\n",
    "\n",
    "`\n",
    "o\n",
    "we\n",
    "all\n",
    "have\n",
    "heard\n",
    "people\n",
    "believe\n",
    "anything\n",
    "`\n",
    "\n",
    "Should look like this in your code:\n",
    "\n",
    "```python\n",
    "\"o\\nwe\\nall\\nhave\\nheard\\npeople\\nbelieve\\nanything\"\n",
    "```\n",
    "\n",
    "Your function should return a string containing your poem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def snowball_generator(start_word, snowball_mapping, longest_natural, use_natural):\n",
    "    ###\n",
    "\n",
    "#     start_word = 'o'\n",
    "    st = start_word\n",
    "    if use_natural:\n",
    "        while len(longest_natural[start_word])>0:\n",
    "            start_word = list(longest_natural[start_word])[-1]\n",
    "            st = st + '\\n' + start_word\n",
    "    else:    \n",
    "        while len(snowball_mapping[start_word])>0:\n",
    "            start_word = random.choice(list(snowball_mapping[start_word]))\n",
    "            st = st + '\\n' + start_word\n",
    "    return st\n",
    "    ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "test_snowball_generator_1",
     "locked": true,
     "points": "2",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First we'll look at your generator linking one word at a time.\n",
      "\n",
      "Let's make some poems about Huckleberry Finn:\n",
      "\n",
      "huck\n",
      "began\n",
      "warily \n",
      "\n",
      "huck\n",
      "found\n",
      "nobody\n",
      "forbade \n",
      "\n",
      "huck\n",
      "finns\n",
      "wealth \n",
      "\n",
      "And now let's make some about Tom Sawyer:\n",
      "\n",
      "tom\n",
      "what\n",
      "owned \n",
      "\n",
      "tom\n",
      "soon\n",
      "weary \n",
      "\n",
      "tom\n",
      "sent\n",
      "forth \n",
      "\n",
      "Finally, let's see what other gems your generator can produce:\n",
      "\n",
      "ma\n",
      "she\n",
      "lost\n",
      "miles\n",
      "around\n",
      "hunting \n",
      "\n",
      "toes\n",
      "stuck\n",
      "around\n",
      "without\n",
      "studying \n",
      "\n",
      "unkempt\n",
      "uncombed \n",
      "\n",
      "fret\n",
      "about\n",
      "snakes\n",
      "created \n",
      "\n",
      "horses\n",
      "leaning \n",
      "\n",
      "orleans\n",
      "intended \n",
      "\n",
      "thick\n",
      "leaves\n",
      "rustled \n",
      "\n",
      "incantation\n",
      "impressively \n",
      "\n",
      "hove\n",
      "those\n",
      "bright \n",
      "\n",
      "blighted\n",
      "condition \n",
      "\n",
      "Why, I feel transported to the Mississippi Delta. Passed!\n"
     ]
    }
   ],
   "source": [
    "#test_snowball_generator_1\n",
    "print(\"First we'll look at your generator linking one word at a time.\\n\")\n",
    "\n",
    "print(\"Let's make some poems about Huckleberry Finn:\\n\")\n",
    "for i in range(3):\n",
    "    huckpoem = snowball_generator(\"huck\", snowball_mapping, longest_natural, use_natural = False)\n",
    "    print(huckpoem, \"\\n\")\n",
    "    hucksplit = huckpoem.split()\n",
    "    assert len(hucksplit[-1]) == len(hucksplit) + 3, \"Looks like the last word of your poem is the wrong length.\"\n",
    "    \n",
    "\n",
    "print(\"And now let's make some about Tom Sawyer:\\n\")\n",
    "for i in range(3):\n",
    "    tompoem = snowball_generator(\"tom\", snowball_mapping, longest_natural, use_natural = False)\n",
    "    print(tompoem, \"\\n\")\n",
    "    tomsplit = tompoem.split()\n",
    "    assert len(tomsplit[-1]) == len(tomsplit) + 2, \"Looks like the last word of your poem is the wrong length.\"\n",
    "\n",
    "print(\"Finally, let's see what other gems your generator can produce:\\n\")\n",
    "for num in range(10):\n",
    "    random_start = random.choice(list(snowball_mapping.keys()))\n",
    "    otherpoem = snowball_generator(random_start, snowball_mapping, longest_natural, use_natural = False)\n",
    "    print(otherpoem, \"\\n\")\n",
    "    othersplit = otherpoem.split()\n",
    "    assert len(othersplit[-1]) == len(othersplit) + len(random_start) - 1, \"Looks like the last word of your poem is the wrong length.\"\n",
    "    \n",
    "print(\"Why, I feel transported to the Mississippi Delta. Passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "test_snowball_generator_2",
     "locked": true,
     "points": "2",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First let's make sure your dictionary has our test words:\n",
      "\n",
      "Looks like they're all there. Now let's make some poems!\n",
      "\n",
      "If \"we\" is the starter word, \"ridiculous\" should be last:\n",
      "\n",
      "we\n",
      "coming\n",
      "himself\n",
      "standing\n",
      "perfectly\n",
      "ridiculous \n",
      "\n",
      "If \"men\" is the starter word, \"clothes\" should be last:\n",
      "\n",
      "men\n",
      "their\n",
      "clothes \n",
      "\n",
      "If \"human\" is the starter word, \"exertion\" should be last:\n",
      "\n",
      "human\n",
      "without\n",
      "exertion \n",
      "\n",
      "If \"lint\" is the starter word, \"dollars\" should be last:\n",
      "\n",
      "lint\n",
      "worth\n",
      "dollars \n",
      "\n",
      "If \"it\" is the starter word, \"stirring\" should be last:\n",
      "\n",
      "it\n",
      "before\n",
      "anybody\n",
      "stirring \n",
      "\n",
      "If \"so\" is the starter word, \"symptoms\" should be last:\n",
      "\n",
      "so\n",
      "could\n",
      "symptoms \n",
      "\n",
      "Passed!\n"
     ]
    }
   ],
   "source": [
    "#test_snowball_generator_2\n",
    "print(\"First let's make sure your dictionary has our test words:\")\n",
    "\n",
    "test_list = [\"we\", \"men\", \"human\", \"lint\", \"it\", \"so\"]\n",
    "\n",
    "for word in test_list:\n",
    "    assert word in longest_natural.keys(), \"Looks like you're missing a word!\"\n",
    "    \n",
    "print(\"\\nLooks like they're all there. Now let's make some poems!\\n\")\n",
    "    \n",
    "last_word_check = [\"ridiculous\", \"clothes\", \"exertion\", \"dollars\", \"stirring\", \"symptoms\"]\n",
    "\n",
    "for num, word in enumerate(test_list):\n",
    "    print('If \"'+ word + '\" is the starter word, \"' + last_word_check[num] + '\" should be last:\\n')\n",
    "    print(snowball_generator(word, snowball_mapping, longest_natural, use_natural = True),\"\\n\")\n",
    "    assert last_word_check[num] in snowball_generator(word, snowball_mapping, longest_natural, use_natural = True), \"Looks like the poem ended with the wrong word!\"\n",
    "\n",
    "print(\"Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "** Fin ** You've reached the end of this problem. Don't forget to restart the kernel and run the entire notebook from top-to-bottom to make sure you did everything correctly. If that is working, try submitting this problem. (Recall that you *must* submit and pass the autograder to get credit for your work.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
