{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Important note**! Before you turn in this lab notebook, make sure everything runs as expected:\n",
    "\n",
    "- First, restart the kernel -- in the menubar, select Kernel → Restart.\n",
    "- Then run all cells -- in the menubar, select Cell → Run All.\n",
    "\n",
    "Make sure you fill in any place that says YOUR CODE HERE or \"YOUR ANSWER HERE.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Problem 4: Data Wrangling for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "In this exercise we will take you through the basics of data cleaning that often is the majority of your work before fitting a training a machine learning model.\n",
    "\n",
    "Data often has a lot of missing values, incorrect data types, rows that need to be removed etc., and this problemgives you a flavor of what is often required before any sort of descriptive, predictive, or prescriptive analysis.\n",
    "\n",
    "For this exercise, we will be using a dataset with credit approval scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "credit = pd.read_csv('creditapproval.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Let us take a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 624 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictor A</th>\n",
       "      <th>Predictor B</th>\n",
       "      <th>Predictor C</th>\n",
       "      <th>Predictor D</th>\n",
       "      <th>Predictor E</th>\n",
       "      <th>Predictor F</th>\n",
       "      <th>Predictor G</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.83</td>\n",
       "      <td>G</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1</td>\n",
       "      <td>t</td>\n",
       "      <td>202.0</td>\n",
       "      <td>Mrketing 1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58.67</td>\n",
       "      <td>J</td>\n",
       "      <td>3.04</td>\n",
       "      <td>6</td>\n",
       "      <td>t</td>\n",
       "      <td>43.0</td>\n",
       "      <td>Mkt6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.50</td>\n",
       "      <td>B</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>280.0</td>\n",
       "      <td>M0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.83</td>\n",
       "      <td>B</td>\n",
       "      <td>3.75</td>\n",
       "      <td>5</td>\n",
       "      <td>f</td>\n",
       "      <td>100.0</td>\n",
       "      <td>Marketing 5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.17</td>\n",
       "      <td>B</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>120.0</td>\n",
       "      <td>Spend 0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Predictor A Predictor B  Predictor C  Predictor D Predictor E  Predictor F  \\\n",
       "0        30.83           G         1.25            1           t        202.0   \n",
       "1        58.67           J         3.04            6           t         43.0   \n",
       "2        24.50           B         1.50            0           f        280.0   \n",
       "3        27.83           B         3.75            5           f        100.0   \n",
       "4        20.17           B         1.71            0           t        120.0   \n",
       "\n",
       "   Predictor G  Response  \n",
       "0   Mrketing 1       1.0  \n",
       "1         Mkt6       1.0  \n",
       "2           M0       1.0  \n",
       "3  Marketing 5       1.0  \n",
       "4      Spend 0       1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The dataset has {} rows.'.format(len(credit))) \n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "The data hence has 7 predictors and 1 response variable.\n",
    "\n",
    "In machine learning or predictive modelling in general, you use predictors (in this case 7 of them) to predict its corresponding response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "But before we can move onto predictive modelling, we need to clean the data. Data cleaning often is the most important part of machine learning and we exactly going to do that bit.\n",
    "\n",
    "For instance, let's check for columns having missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Predictor A     True\n",
       "Predictor B     True\n",
       "Predictor C    False\n",
       "Predictor D    False\n",
       "Predictor E    False\n",
       "Predictor F     True\n",
       "Predictor G    False\n",
       "Response        True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "So the columns **Predictor A, Predictor B, Predictor F, and Response** all have missing values. You'll treat these cases through the exercises below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Data without a **Response variable** can neither be used in training the model or in testing it. We would hence like to remove rows that have **both Predictor F** and **Response as NaN values**\n",
    "\n",
    "\n",
    "**Exercise 0** (1 point): Create a new dataframe named **`creditwithresponse`** that is a copy of **`credit`** but with any rows missing **either** `Predictor F` **or** `Response` removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Exercise0Response",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "612"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###\n",
    "creditwithresponse = credit.copy()\n",
    "\n",
    "creditwithresponse.drop(creditwithresponse[(creditwithresponse['Predictor F'].isnull()) | (creditwithresponse['Response'].isnull())].index,inplace = True)\n",
    "\n",
    "len(creditwithresponse)\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Exercise 0",
     "locked": true,
     "points": "1",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Passed!)\n"
     ]
    }
   ],
   "source": [
    "##Test cell: Exercise 0\n",
    "assert len(creditwithresponse) == 612, \"The length of your newly created dataframe does not match the solution\"\n",
    "assert len(creditwithresponse[creditwithresponse['Predictor F'].isnull()]) == 0, \"Some NaN values still exist in your new dataframe.\"\n",
    "assert len(creditwithresponse[creditwithresponse['Response'].isnull()]) == 0, \"Almost there! Though some NaN values still exist in your new dataframe.\"\n",
    "\n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "What about the other predictors?\n",
    "\n",
    "One technique is to replace missing values with sensible substitutes. For instance, we might replace a missing value with the **mean** of the remaining values in the case of a numerical variable, or the **mode** in the case of a categorical (discrete) variable.\n",
    "\n",
    "So, for instance, suppose a numerical predictor has the values `[1.0, 6.5, 3.5, NaN, 5.0]`. Then, you might replace the `NaN` with the mean of the known values, `[1.0, 6.5, 3.5, 5.0]`, which is 4.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Exercise 1 (3 points)**: Create a function called **`imputevalue()`** that takes, as its inputs, a dataframe, the name of a column in that dataframe, and the replacement method. The replacement method will be a string, either `\"mean\"` or `\"mode\"`.\n",
    "\n",
    "With these three inputs, your function should do the following:\n",
    "\n",
    "1. Create a copy of the dataframe (i.e., the original should remain intact).\n",
    "2. Compute the **mean** or **mode** of the column **without** the NaN values.\n",
    "3. Replace the NaN's in that column with the computed mean/mode.\n",
    "4. Return this new dataframe (i.e., not just the column containing the newly imputed values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Exercise1Response",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def imputevalue(df, col, func):\n",
    "    assert func in ['mean', 'mode'], \"You might have edited the assertion in this code cell, please reload this cell\"\n",
    "\n",
    "    ###\n",
    "    dfc = df.copy()\n",
    "    \n",
    "    if func == 'mean':\n",
    "        dfc[col].fillna(dfc[col].mean(), inplace=True)\n",
    "    else:\n",
    "        dfc[col].fillna(dfc[col].mode()[0], inplace=True)\n",
    "    \n",
    "    return dfc\n",
    "    ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Exercise 1",
     "locked": true,
     "points": "3",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Passed!)\n"
     ]
    }
   ],
   "source": [
    "##Test cell: Exercise 1\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "df2 = imputevalue(creditwithresponse, 'Predictor A', 'mean')\n",
    "assert not(df2.equals(creditwithresponse)), 'You have not created a copy of the dataframe'\n",
    "assert (round(np.mean(df2['Predictor A']), 2) >= 31.8) & (round(np.mean(df2['Predictor A']), 2)<=31.9), \"The imputed value is incorrect. Please check your code\"\n",
    "\n",
    "df2 = imputevalue(creditwithresponse, 'Predictor B', 'mode')\n",
    "assert df2.loc[:,'Predictor B'].mode()[0] == 'B', \"The imputed value is incorrect. Please check your code\"\n",
    "\n",
    "credit_imputed_temp = imputevalue(creditwithresponse, 'Predictor A', 'mean')\n",
    "credit_imputed = imputevalue(credit_imputed_temp, 'Predictor B', 'mode')\n",
    "\n",
    "assert credit_imputed['Predictor A'].notnull().all()==True, 'There are still some missing values in Predictor A'\n",
    "assert credit_imputed['Predictor B'].notnull().all()==True, 'There are still some missing values in Predictor B'\n",
    "\n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Using the preceding techniques (removing missing rows or imputing values), we've covered all variables except `Predictor G`. Let's treat that one next. First, let's inspect it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Mrketing 1\n",
       "1           Mkt6\n",
       "2             M0\n",
       "3    Marketing 5\n",
       "4        Spend 0\n",
       "Name: Predictor G, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_imputed['Predictor G'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "This column actually contains marketing expenditures in thousands of dollars. For example, `'Marketing 1'` means that a total of $1000 was spent on this marketing campaign.\n",
    "\n",
    "As you can see, these data were not entered in a consistent way, except that a numerical value does appear. In this exercise you are required to extract the numbers from the column's values, e.g., extract **`1`** from `'Marketing 1'`.\n",
    "\n",
    "Please note that the following facts about the values in the column 'Predictor G'.\n",
    "1. Each value begins with a string of alphabetic characters. This string may vary from row to row.\n",
    "2. A space may or may not follow that initial string of alphabetic characters.\n",
    "3. The string ends with a sequence of digits.\n",
    "\n",
    "Refer to the sample values from the call to `.head()` above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Exercise 2 (3 points)**: Create a function **`strip_text()`** that takes a **`(dataframe, column)`** as inputs and returns a **dataframe** according to the desciption below.\n",
    "\n",
    "With these two inputs, your function should:\n",
    "\n",
    "1. Create a copy of the dataframe, i.e., the original should remain intact.\n",
    "2. For the given column, remove all the text in the column so that it contains only numbers (integers).\n",
    "3. Return this new dataframe, i.e., not just the column containing the newly imputed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Exercise2Response",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def strip_text(df, col):\n",
    "    ###\n",
    "\n",
    "    dfc = df.copy()\n",
    "    \n",
    "    dfc[col] = dfc[col].str.replace('[a-zA-Z]+','')\n",
    "    dfc[col] = dfc[col].str.strip().astype(int)\n",
    "    \n",
    "    \n",
    "    return dfc\n",
    "    \n",
    "    ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Exercise 2",
     "locked": true,
     "points": "3",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col1    int64\n",
      "dtype: object\n",
      "col1    int64\n",
      "dtype: object\n",
      "   col1\n",
      "0     0\n",
      "1     2\n",
      "2   123\n",
      "3    12\n",
      "4   999\n",
      "5    12\n",
      "6    12\n",
      "   col1\n",
      "0     0\n",
      "1     2\n",
      "2   123\n",
      "3    12\n",
      "4   999\n",
      "5    12\n",
      "6    12\n",
      "\n",
      "(Passed!)\n"
     ]
    }
   ],
   "source": [
    "##Test cell: Exercise 2\n",
    "\n",
    "instr = pd.DataFrame(['Rich0','Rachel    2', 'Sam123', 'Ben 012', 'Evan 999', 'Chinmay12', '   Raghav12'])\n",
    "instr2 = instr.rename(columns={0:'col1'})\n",
    "df1 = strip_text(instr2, 'col1')\n",
    "df2 = pd.DataFrame([0,2,123,12,999,12,12]).rename(columns={0:'col1'})\n",
    "print(df1.dtypes)\n",
    "print(df2.dtypes)\n",
    "# merged = df1.merge(df2, indicator=True, how='outer')\n",
    "# merged[merged['_merge'] == 'left_only']\n",
    "print(df1)\n",
    "print(df2)\n",
    "df1.equals(df2)\n",
    "# assert strip_text(instr2,'col1').equals(pd.DataFrame([0,2,123,12,999,12,12]).rename(columns={0:'col1'})),\"Please check your output by running your function on the 'instr' dataframe\"\n",
    "\n",
    "credit_cleaned = strip_text(credit_imputed,'Predictor G')\n",
    "assert not(credit_cleaned.equals(credit_imputed)), 'You have not created a copy of the dataframe'\n",
    "assert credit_cleaned['Predictor G'].dtype  == 'int64', \"Output data type does not match\"\n",
    "assert len(credit_cleaned) == 612, \"Your dataframe output is not of the appropriate length\"\n",
    "assert (round(np.mean(credit_cleaned['Predictor G']),2) >= 2.62) & (round(np.mean(credit_cleaned['Predictor G']),2)<=2.64), \"The imputed data does not match. You could try replicating these tests on the 'instr' dataframe above.\"\n",
    "assert (round(np.sum(credit_cleaned['Predictor G']),2) >= 1611.0) & (round(np.sum(credit_cleaned['Predictor G']),2) <= 1613.0) , \"The imputed data does not match. You could try replicating these tests on the 'instr' dataframe above.\"\n",
    "\n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Now that you have cleaned your dataset, let's do one final check to see if we still have any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Predictor A    False\n",
       "Predictor B    False\n",
       "Predictor C    False\n",
       "Predictor D    False\n",
       "Predictor E    False\n",
       "Predictor F    False\n",
       "Predictor G    False\n",
       "Response       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_cleaned.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "You should see all `False` values, meaning there is no missing data in any of the columns. If so, great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Creating Interaction Terms in the Data**\n",
    "\n",
    "Sometimes, for analysis purposes, it is better to create _interaction predictors_, which are new predictors that modify or combine existing predictors. For example, in a marketing scenario, spending on TV marketing might have a quadratic relationship with the sales of the product. We would hence want to include ** $(\\mathrm{TV\\ marketing})^2$ **  as a predictor to better capture the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "In this final exercise we will create a new predictor that is a combination of the predictors in the dataset **`credit_cleaned`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Exercise 3 (3 points):** Create a function **`familiarity()`** that takes as its inputs a dataframe (`df`), the names of three input columns (`column1`, `column2`, `column3`), and the name of a new output column (`columnnew`). It should compute for the values of this new column what appears in the formula below.\n",
    "\n",
    "**$$\\mathtt{columnnew} = \\frac{\\mathtt{column1}}{e^{\\mathtt{column2}}} - \\sqrt{\\mathtt{column3}},$$**\n",
    "\n",
    "where **$$\\sqrt{\\mathtt{column3}} = (\\mathtt{column3})^{0.5}.$$**\n",
    "\n",
    "The return value for the function will be a dataframe with the new column, **`columnnew`**, **in addition** to all the original columns in the dataframe.\n",
    "\n",
    "> **Note.** If a value in column 3 is negative, so that the square-root is undefined, set the corresponding value in `columnnew` to zero (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "Exercise3Response",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def familiarity(df, column1, column2, column3, columnnew):\n",
    "    ###\n",
    "    \n",
    "    df[columnnew] = 0\n",
    "    df.loc[df[column3] > 0, columnnew] = (df.loc[df[column3]> 0, column3])**0.5 \n",
    "    \n",
    "    df[columnnew] =  df[column1] / np.exp(df[column2]) - df[columnnew]\n",
    "    df.loc[df[column3] < 0, columnnew] = 0\n",
    "    return df\n",
    "    ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Exercise 3",
     "locked": true,
     "points": "3",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Passed!)\n"
     ]
    }
   ],
   "source": [
    "##Test cell: Exercise 3\n",
    "\n",
    "d={'col1':[1,2,3,4,5], 'col2':[2,3,4,0,4], 'col3':[-9,2,-8,0,0]}\n",
    "df = pd.DataFrame(d)\n",
    "dffamiliarity = familiarity(df, 'col1', 'col2', 'col3', 'colnew')\n",
    "assert dffamiliarity.loc[dffamiliarity['col3']<0,'colnew'].all()==0, \"The non negative case for col3 is failing. Please check your code\"\n",
    "\n",
    "\n",
    "credit_final = familiarity(credit_cleaned, 'Predictor A', 'Predictor C', 'Predictor G', 'Predictor H')\n",
    "assert 'Predictor H' in credit_final, \"Column 'Predictor H' does not exist\"\n",
    "assert len(credit_final) == 612, \"The length of the dataframe does not match the required length\"\n",
    "assert (round(np.sum(credit_final['Predictor H']),2) >= 7262.4) & (round(np.sum(credit_final['Predictor H']),2) <= 7262.5), \"The sum of values in Predictor H do not match the required vlue\"\n",
    "\n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "At this point, you have completed all the exercises and can go ahead and submit the notebook!\n",
    "\n",
    "However, we have however added a small piece of code below to give you an idea of how simple it is to create a predicitive model in Python. It is **not graded** and hence you can submit this notebook, complete other notebooks and come back and have a look at it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing accuracy of the random forest classifier is:  0.7398373983739838\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Split the dataset into predictors and response\n",
    "datax = credit_final[['Predictor A', 'Predictor C', 'Predictor D', 'Predictor F', 'Predictor G', 'Predictor H']]\n",
    "datay = credit_final[['Response']]\n",
    "\n",
    "#Traintest split - Test sets are created to test the accuracy of your model on a piece of data that is not used to train the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(datax, np.ravel(datay), test_size=0.20, random_state=42)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=500) #Number of decision trees in the forest = 500\n",
    "forest.fit(X_train,y_train) #Train the classifier using the train data\n",
    "forest_pred = forest.predict(X_test) #Predict the classes for the test data\n",
    "print(\"The testing accuracy of the random forest classifier is: \",accuracy_score(y_test, forest_pred)) #Print the accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "The accuracy of the model above is about 70% or so. Due to the nature of the dataset being artificial, we don't expect a higher accuracy. Instead, our purpose here is to give you an idea as to how easy it is to do predictive modelling (machine learning) in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "\n",
    "**Fin!** That's the end of this problem. Don't forget to restart and run this notebook from the beginning to verify that it works top-to-bottom before submitting. You can move on to the next problem"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
